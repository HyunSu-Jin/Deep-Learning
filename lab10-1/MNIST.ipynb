{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Cost :  0.485057080137\n",
      "Epoch 2 Cost :  0.17492504724\n",
      "Epoch 3 Cost :  0.133461480879\n",
      "Epoch 4 Cost :  0.112356460718\n",
      "Epoch 5 Cost :  0.0933711894949\n",
      "Epoch 6 Cost :  0.0852105632915\n",
      "Epoch 7 Cost :  0.0762664886475\n",
      "Epoch 8 Cost :  0.067546607935\n",
      "Epoch 9 Cost :  0.0645035094332\n",
      "Epoch 10 Cost :  0.0584281745794\n",
      "Epoch 11 Cost :  0.0562146977568\n",
      "Epoch 12 Cost :  0.0533668008605\n",
      "Epoch 13 Cost :  0.0500391882224\n",
      "Epoch 14 Cost :  0.0502267064656\n",
      "Epoch 15 Cost :  0.0471721950952\n",
      "Accuracy :  0.9813\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(777)  # reproducibility\n",
    "nb_classes = 10\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "X = tf.placeholder(tf.float32,shape=(None,784))\n",
    "Y = tf.placeholder(tf.float32,shape=(None,nb_classes))\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# layer1\n",
    "with tf.name_scope(\"layer1\") as scope:\n",
    "    W1 = tf.get_variable(\"W1\",shape=(784,512),initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.random_normal([512]),name=\"bias1\")\n",
    "    L1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "    L1 = tf.nn.dropout(L1,keep_prob=keep_prob)\n",
    "    \n",
    "    w1_hist = tf.summary.histogram(\"weights1\", W1)\n",
    "    b1_hist = tf.summary.histogram(\"biases1\", b1)\n",
    "    layer1_hist = tf.summary.histogram(\"layer1\", L1)\n",
    "\n",
    "# layer2\n",
    "with tf.name_scope(\"layer2\") as scope:\n",
    "    W2 = tf.get_variable(\"W2\",shape=(512,512),initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.random_normal([512]),name=\"bias2\")\n",
    "    L2 = tf.nn.relu(tf.matmul(L1,W2)+b2)\n",
    "    L2 = tf.nn.dropout(L2,keep_prob=keep_prob)\n",
    "    \n",
    "    w2_hist = tf.summary.histogram(\"weights2\", W2)\n",
    "    b2_hist = tf.summary.histogram(\"biases2\", b2)\n",
    "    layer2_hist = tf.summary.histogram(\"layer2\", L2)\n",
    "    \n",
    "# layer3\n",
    "with tf.name_scope(\"layer3\") as scope:\n",
    "    W3 = tf.get_variable(\"W3\",shape=(512,512),initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.Variable(tf.random_normal([512]),name=\"bias3\")\n",
    "    L3 = tf.nn.relu(tf.matmul(L2,W3)+b3)\n",
    "    L3 = tf.nn.dropout(L3,keep_prob=keep_prob)\n",
    "    \n",
    "    w3_hist = tf.summary.histogram(\"weights3\", W3)\n",
    "    b3_hist = tf.summary.histogram(\"biases3\", b3)\n",
    "    layer3_hist = tf.summary.histogram(\"layer3\", L3)\n",
    "\n",
    "# layer4\n",
    "with tf.name_scope(\"layer4\") as scope:\n",
    "    W4 = tf.get_variable(\"W4\",shape=(512,512),initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b4 = tf.Variable(tf.random_normal([512]),name=\"bias4\")\n",
    "    L4 = tf.nn.relu(tf.matmul(L3,W4)+b4)\n",
    "    L4 = tf.nn.dropout(L4,keep_prob=keep_prob)\n",
    "    \n",
    "    w4_hist = tf.summary.histogram(\"weights4\", W4)\n",
    "    b4_hist = tf.summary.histogram(\"biases4\", b4)\n",
    "    layer4_hist = tf.summary.histogram(\"layer4\", L4)\n",
    "\n",
    "# output layer\n",
    "with tf.name_scope(\"layer5\") as scope:\n",
    "    W5 = tf.get_variable(\"W5\",shape=(512,nb_classes),initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b5 = tf.Variable(tf.random_normal([nb_classes]),name=\"bias5\")\n",
    "    hypothesis = tf.matmul(L4,W5) +b5\n",
    "    \n",
    "    w5_hist = tf.summary.histogram(\"weights5\", W5)\n",
    "    b5_hist = tf.summary.histogram(\"biases5\", b5)\n",
    "    hypothesis_hist = tf.summary.histogram(\"hypothesis\", hypothesis)\n",
    "\n",
    "#cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis),axis=1))\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis,labels=Y)\n",
    "with tf.name_scope(\"cost\") as scope:\n",
    "    cost = tf.reduce_mean(cost_i)\n",
    "    cost_scalar = tf.summary.scalar(\"cost\",cost)\n",
    "\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "predicted = tf.arg_max(hypothesis,dimension=1)\n",
    "is_correct= tf.equal(predicted,tf.arg_max(Y,dimension=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,dtype=tf.float32))\n",
    "accuracy_scalar = tf.summary.scalar(\"accuracy\",accuracy)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./logs/EXAMPLE\")\n",
    "    writer.add_graph(sess.graph) # Show the graph\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size) # 한 epoch에 몇번의 batch를 실행해야 하는지\n",
    "        for step in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            cost_val,summary,_ = sess.run([cost,merged_summary,train],feed_dict={\n",
    "                X : batch_xs,\n",
    "                Y : batch_ys,\n",
    "                keep_prob : 0.7\n",
    "            })\n",
    "            writer.add_summary(summary, global_step=step)\n",
    "            avg_cost += cost_val / total_batch\n",
    "        print(\"Epoch\",epoch+1,\"Cost : \",avg_cost)\n",
    "    \n",
    "    accuracy = sess.run(accuracy,feed_dict={\n",
    "        X : mnist.test.images,\n",
    "        Y : mnist.test.labels,\n",
    "        keep_prob : 1.0\n",
    "    })\n",
    "    print(\"Accuracy : \",accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
